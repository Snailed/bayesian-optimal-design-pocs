{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "667271f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from autograd.scipy.linalg import logm\n",
    "from autograd import grad, jacobian, hessian\n",
    "import numpy\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68b0f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_q_params(encoded_q):\n",
    "        shape = len(encoded_q)\n",
    "        mean_shape = int(shape/3)\n",
    "        A_shape = (int(np.sqrt(shape - mean_shape)), int(np.sqrt(shape - mean_shape)))\n",
    "        mean = encoded_q[0:mean_shape]\n",
    "        A = encoded_q[mean_shape:shape].reshape(A_shape)\n",
    "        return mean, A\n",
    "def encode_q_params(q_params):\n",
    "    mean, A = q_params\n",
    "    return np.array(list(mean) + list(A.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8bb54766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(x0, gradient, callback=lambda x: x, rate=0.5, iters=1e1):\n",
    "    x = x0\n",
    "    running_average = []\n",
    "    for i in range(int(iters)):\n",
    "        r = rate/(1+i*1e-1)\n",
    "        g = gradient(x)\n",
    "        x = -g * r + x\n",
    "        if i % 1 == 0:\n",
    "            if not callback(x):\n",
    "                break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9502a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(theta_i, q_params):\n",
    "    mean, A = q_params\n",
    "    return scipy.stats.multivariate_normal.logpdf(theta_i, mean=mean, cov=A @ A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2149527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(q_params, d, y_i):\n",
    "    def stable_multivariate_gaussian_logpdf(x, mu, cov):\n",
    "        n = len(x)\n",
    "        x_mu = x - mu\n",
    "        try:\n",
    "            _, log_det = np.linalg.slogdet(cov)\n",
    "            cov_inv = np.linalg.inv(cov)\n",
    "            log_prob = -0.5 * (n * np.log(2 * np.pi) + log_det + np.dot(x_mu, np.dot(cov_inv, x_mu)))\n",
    "        except np.linalg.LinAlgError:\n",
    "            cov_modified = cov + np.eye(n) * 1e-8\n",
    "            _, log_det = np.linalg.slogdet(cov_modified)\n",
    "            cov_inv = np.linalg.inv(cov_modified)\n",
    "            log_prob = -0.5 * (n * np.log(2 * np.pi) + log_det + np.dot(x_mu, np.dot(cov_inv, x_mu)))\n",
    "        return log_prob\n",
    "    def log_likelihood(y, theta, d):\n",
    "        likelihood_cov = np.mean(np.square(y - theta.T @ d.T)) * np.eye(len(d))\n",
    "        likelihood_mean = d @ theta\n",
    "        return stable_multivariate_gaussian_logpdf(y, likelihood_mean, likelihood_cov)\n",
    "    def KLD(mean_q, A_q, mean_p, A_p):\n",
    "        sigma_q = (A_q @ A_q.T)\n",
    "        sigma_p = (A_p @ A_p.T)\n",
    "        bar_sigma_q = np.linalg.norm(sigma_q)\n",
    "        bar_sigma_p = np.linalg.norm(sigma_p)\n",
    "        k = len(mean_q)\n",
    "        return 0.5 * (np.trace(np.linalg.inv(sigma_p) @ sigma_q) + (mean_p - mean_q).T @ np.linalg.inv(sigma_p) @ (mean_p - mean_q) - k + np.log(bar_sigma_p/bar_sigma_q))\n",
    "    mean, A = q_params\n",
    "    if len(d.shape) == 1:\n",
    "        shape = len(d)\n",
    "    else:\n",
    "        shape = d.shape[1]\n",
    "    values = []\n",
    "    while len(values) < 100:\n",
    "        sample = np.random.multivariate_normal(np.zeros(shape), np.eye(shape))\n",
    "        theta = mean + A @ sample\n",
    "        lik = log_likelihood(y_i, theta, d)\n",
    "        values.append(lik)\n",
    "    return 1/(len(values))*np.sum(values) - KLD(mean, A, mean_prior, A_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e5f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_grad(theta_i, y_i, d, q_params):\n",
    "    def training_hessian_inner(q_params):\n",
    "        return elbo(decode_q_params(q_params), d, y_i)\n",
    "    def training_mixed_partials_inner(encoded_q):\n",
    "        print(\"inner result: %s\" % str(grad(lambda d: elbo(decode_q_params(encoded_q), d, y_i))(d)))\n",
    "        return grad(lambda d: elbo(decode_q_params(encoded_q), d, y_i))(d)\n",
    "    training_hessian = hessian(training_hessian_inner)\n",
    "    training_mixed_partials = jacobian(training_mixed_partials_inner)\n",
    "    encoded_q = encode_q_params(q_params)\n",
    "    latest_hessian = training_hessian(encoded_q)\n",
    "    for i in range(10):\n",
    "        if np.linalg.det(latest_hessian) != 0:\n",
    "            print(\"Encoded q: %s\" % str(encoded_q))\n",
    "            print(\"Mixed Partials: %s\" % str(training_mixed_partials(encoded_q)))\n",
    "            return - np.linalg.inv(latest_hessian) @ training_mixed_partials(encoded_q)\n",
    "        latest_hessian = latest_hessian + 1e-8 * np.eye(latest_hessian.shape)\n",
    "    raise ValueError(\"Was not able to invert hessian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a8d18952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get returns mean, A for log q to approximate log_posterior\n",
    "def optimal_q(d, y_i):\n",
    "    thetas = []\n",
    "    def objective_f(encoded_q):\n",
    "        q_params = decode_q_params(encoded_q)\n",
    "        return elbo(q_params, y_i, d)\n",
    "    def callback(qi):\n",
    "        mean, A = decode_q_params(qi)\n",
    "        print(\"\\t New ELBO iteration. mean=%s, A=%s\" % (str(mean), str(A)))\n",
    "    results = optimizer(encode_q_params((mean_prior, A_prior)), grad(objective_f), callback=callback)\n",
    "    return decode_q_params(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40e6ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.2\n",
    "def MI_grad(d):\n",
    "    size = d.shape[0]\n",
    "    N = 10\n",
    "    theta_samples = np.random.multivariate_normal(mean_prior, A_prior, size=10)\n",
    "    sample_results = []\n",
    "    for i in range(N):\n",
    "        theta_i = theta_samples[i]\n",
    "        z_i = np.random.multivariate_normal(np.zeros(size), noise * np.eye(size))\n",
    "        y_i = theta_i.T @ d + z_i\n",
    "        q_params = optimal_q(d, y_i)\n",
    "        result = log_posterior_grad(theta_i, y_i, d, q_params) * (log_posterior(theta_i, q_params) + 1)\n",
    "        samples_results.append(result)\n",
    "    return np.mean(sample_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7431fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t New ELBO iteration. mean=[2.451713   5.44658235], A=[[3.4488981  1.5552849 ]\n",
      " [1.06532732 3.14431718]]\n",
      "Encoded q: [2.451713   5.44658235 3.4488981  1.5552849  1.06532732 3.14431718]\n",
      "inner result: Autograd ArrayBox with value [[ 0.30096467  0.52133202]\n",
      " [-0.15433996 -0.11083063]]\n",
      "Mixed Partials: [[[ 0.07926791 -0.01747789 -0.00346955 -0.03300282 -0.01990339\n",
      "   -0.02601042]\n",
      "  [-0.00447041  0.09227538 -0.03850272 -0.03499761 -0.02206888\n",
      "   -0.04199001]]\n",
      "\n",
      " [[-0.02950488 -0.03594928 -0.06395816 -0.02976289 -0.02749807\n",
      "    0.00964318]\n",
      "  [-0.04753503 -0.04109063 -0.0193198  -0.0074582  -0.05577989\n",
      "   -0.04686427]]]\n",
      "inner result: Autograd ArrayBox with value [[ 0.31458068  0.59370235]\n",
      " [-0.27466459 -0.26361711]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(di):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew MI iteration. di=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(di))\n\u001b[0;32m----> 6\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMI_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 6\u001b[0m, in \u001b[0;36moptimizer\u001b[0;34m(x0, gradient, callback, rate, iters)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(iters)):\n\u001b[1;32m      5\u001b[0m     r \u001b[38;5;241m=\u001b[39m rate\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mg \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[110], line 12\u001b[0m, in \u001b[0;36mMI_grad\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     10\u001b[0m     y_i \u001b[38;5;241m=\u001b[39m theta_i\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m d \u001b[38;5;241m+\u001b[39m z_i\n\u001b[1;32m     11\u001b[0m     q_params \u001b[38;5;241m=\u001b[39m optimal_q(d, y_i)\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlog_posterior_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (log_posterior(theta_i, q_params) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     samples_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(sample_results)\n",
      "Cell \u001b[0;32mIn[114], line 15\u001b[0m, in \u001b[0;36mlog_posterior_grad\u001b[0;34m(theta_i, y_i, d, q_params)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded q: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(encoded_q))\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixed Partials: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(training_mixed_partials(encoded_q)))\n\u001b[0;32m---> 15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest_hessian\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_mixed_partials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     latest_hessian \u001b[38;5;241m=\u001b[39m latest_hessian \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(latest_hessian\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas not able to invert hessian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 6)"
     ]
    }
   ],
   "source": [
    "d0 = np.array([[1,1], [2,2]])\n",
    "A_prior = np.array([[3.2, 1.6], [1.1, 2.9]])\n",
    "mean_prior = np.array([2, 5])\n",
    "def callback(di):\n",
    "    print(\"New MI iteration. di=%s\" % str(di))\n",
    "optimizer(d0, MI_grad, callback=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
