{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd878c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from autograd.scipy.linalg import logm\n",
    "from autograd import grad, jacobian, hessian\n",
    "import numpy\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f540d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_q_params(encoded_q):\n",
    "        shape = len(encoded_q)\n",
    "        mean_shape = 3\n",
    "        A_shape = (int(np.sqrt(shape - mean_shape)), int(np.sqrt(shape - mean_shape)))\n",
    "        mean = encoded_q[0:mean_shape]\n",
    "        A = encoded_q[mean_shape:shape].reshape(A_shape)\n",
    "        return mean, A\n",
    "def encode_q_params(q_params):\n",
    "    mean, A = q_params\n",
    "    return np.array(list(mean) + list(A.flatten()))\n",
    "\n",
    "def decode_d(encoded_d, dim=2):\n",
    "    return encoded_d.reshape(int(len(encoded_d)/dim), dim)\n",
    "def encode_d(d):\n",
    "    return d.flatten()\n",
    "\n",
    "def augment_d(d):\n",
    "    return np.concatenate((d.T, [np.ones(len(d))]), axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b95de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6166fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_multivariate_gaussian_logpdf(x, mu, cov):\n",
    "    n = cov.shape[-1]\n",
    "    x_mu = x - mu\n",
    "    try:\n",
    "        _, log_det = np.linalg.slogdet(cov)\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "        prod = np.einsum('...i, ...ij, ...j->...', x_mu, cov_inv, x_mu) # todo: find a way to make this understandable\n",
    "        log_prob = -0.5 * (n * np.log(2 * np.pi) + log_det + prod)\n",
    "    except np.linalg.LinAlgError:\n",
    "        cov_modified = cov + np.eye(n) * 1e-8\n",
    "        _, log_det = np.linalg.slogdet(cov_modified)\n",
    "        cov_inv = np.linalg.inv(cov_modified)\n",
    "        prod = np.einsum('...i, ...ij, ...j->...', x_mu, cov_inv, x_mu)\n",
    "        log_prob = -0.5 * (n * np.log(2 * np.pi) + log_det + prod)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdbc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(d, y_i):\n",
    "    d = np.concatenate((d, np.array([np.ones(d.shape[0])]).T), axis=1)\n",
    "    var_y = np.var(y_i) * np.eye(len(y_i))\n",
    "    covariance_prior = A_prior @ A_prior.T\n",
    "    mean_theta = covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ y_i\n",
    "    cov_theta = covariance_prior - covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ d @ covariance_prior\n",
    "    return mean_theta, np.linalg.cholesky(cov_theta)\n",
    "def get_posteriors(d, y):\n",
    "    d = augment_d(d)\n",
    "    var_y = noise * np.eye(y.shape[1])\n",
    "    covariance_prior = A_prior @ A_prior.T\n",
    "    mean_theta = covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ y.T\n",
    "    cov_theta = covariance_prior - covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ d @ covariance_prior\n",
    "    return mean_theta, np.linalg.cholesky(cov_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbbffdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d0 = np.random.randn(20,2)\n",
    "d0 = np.array([[0,0] for i in range(10)])\n",
    "#A_prior = np.array([[5, -2, 1],\n",
    "#              [-2, 4, -1],\n",
    "#              [1, -1, 3]])\n",
    "A_prior = np.eye(3)\n",
    "mean_prior = np.array([0,0, 0])\n",
    "noise = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642a3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.        ,  0.        , -1.57067205]), array([[1.        , 0.        , 0.        ],\n",
      "       [0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 0.28668588]]))\n",
      "(array([[ 0.       ,  0.       ],\n",
      "       [ 0.       ,  0.       ],\n",
      "       [-1.5557488, -1.7420673]]), array([[1.        , 0.        , 0.        ],\n",
      "       [0.        , 1.        , 0.        ],\n",
      "       [0.        , 0.        , 0.30151134]]))\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.multivariate_normal(mean_prior, A_prior @ A_prior.T)\n",
    "theta2 = np.random.multivariate_normal(mean_prior, A_prior @ A_prior.T)\n",
    "\n",
    "z = np.random.randn(d0.shape[0]) * noise\n",
    "z2 = np.random.randn(d0.shape[0]) * noise\n",
    "print(get_posterior(d0, theta @ augment_d(d0).T + z))\n",
    "print(get_posteriors(np.array(d0), np.array([theta @ augment_d(d0).T + z, theta2 @ augment_d(d0).T + z2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf7a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MI(d, N=50):\n",
    "    thetas = np.random.multivariate_normal(mean_prior, A_prior @ A_prior.T, size=N)\n",
    "    z = np.random.randn(N, d.shape[0]) * noise\n",
    "    y = thetas @ augment_d(d).T + z\n",
    "    mean, A = get_posteriors(d, y)\n",
    "    #print(mean.T)\n",
    "    results = [stable_multivariate_gaussian_logpdf(theta_i, mean_i, A @ A.T) for theta_i, mean_i in zip(thetas, mean.T)]\n",
    "    return 1/N * np.sum(results) - 0.5 * np.log(np.linalg.det(2 * np.pi * np.e * (A_prior @ A_prior.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2a5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_prior = A_prior @ A_prior.T\n",
    "def mutual_information(d):\n",
    "    N = 10 # amount of theta samples\n",
    "    M = 5 # amount of z samples\n",
    "    thetas = np.random.multivariate_normal(mean_prior, cov_prior, N)\n",
    "    results = []\n",
    "    for theta in thetas:\n",
    "        zs = np.random.randn(M)\n",
    "        ys = np.array([augment_d(d) @ theta + noise * z for theta in thetas for z in zs])\n",
    "        for y in ys:\n",
    "            mean, A = get_posterior(d, y)\n",
    "            log_posterior = stable_multivariate_gaussian_logpdf(theta, mean, A @ A.T) # using posterior_distribution from last week\n",
    "            log_prior = stable_multivariate_gaussian_logpdf(theta, mean_prior, cov_prior)\n",
    "            results.append(log_posterior - log_prior)\n",
    "    return 1/(N * M) * np.sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d315ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLD(mean_q, A_q, mean_p, A_p):\n",
    "        sigma_q = (A_q @ A_q.T)\n",
    "        sigma_p = (A_p @ A_p.T)\n",
    "        bar_sigma_q = np.linalg.norm(sigma_q)\n",
    "        bar_sigma_p = np.linalg.norm(sigma_p)\n",
    "        k = len(mean_q)\n",
    "        return 0.5 * (np.trace(np.linalg.inv(sigma_p) @ sigma_q) + (mean_p - mean_q).T @ np.linalg.inv(sigma_p) @ (mean_p - mean_q) - k + np.log(bar_sigma_p/bar_sigma_q))\n",
    "def KLD_opt(mean_q, A_q, mean_p, A_p):\n",
    "    # Compute Cholesky decomposition\n",
    "    L_p = np.linalg.cholesky(A_p @ A_p.T)\n",
    "    L_q = np.linalg.cholesky(A_q @ A_q.T)\n",
    "\n",
    "    # Compute trace term\n",
    "    trace_term = np.sum(np.log(np.diag(L_p))) - np.sum(np.log(np.diag(L_q)))\n",
    "\n",
    "    # Compute mean difference term\n",
    "    mean_diff = mean_p - mean_q\n",
    "    mean_diff_term = np.dot(np.linalg.solve(L_p.T, np.linalg.solve(L_p, mean_diff)), mean_diff)\n",
    "\n",
    "    # Compute bar_sigma_p and bar_sigma_q\n",
    "    bar_sigma_q = np.linalg.norm(L_q)\n",
    "    bar_sigma_p = np.linalg.norm(L_p)\n",
    "\n",
    "    k = len(mean_q)\n",
    "\n",
    "    return 0.5 * (trace_term + mean_diff_term - k + np.log(bar_sigma_p / bar_sigma_q))\n",
    "def log_likelihood_opt(y, theta, d):\n",
    "    d_offset = np.concatenate((d.T, np.ones((1, len(d)))), axis=0).T\n",
    "    likelihood_cov = np.mean(np.square(y - np.matmul(theta.T, d_offset.T)), axis=1)[:, np.newaxis, np.newaxis] * np.eye(len(d))[np.newaxis, ...]\n",
    "    likelihood_mean = np.matmul(theta.T, d_offset.T)\n",
    "    log_likelihoods = stable_multivariate_gaussian_logpdf(y, likelihood_mean, likelihood_cov)\n",
    "    return log_likelihoods\n",
    "def log_likelihood(y, theta, d):\n",
    "    #print(np.array([np.eye(len(d)) for _ in theta.T]).shape)\n",
    "    d_offset = np.concatenate((d.T, [np.ones(len(d))]), axis=0).T\n",
    "    #print(np.mean(np.square(y - theta.T @ d.T), axis=1))\n",
    "    #print(d)\n",
    "    likelihood_cov = np.mean(np.square(y - theta.T @ d_offset.T), axis=1)[:, np.newaxis, np.newaxis] * np.array([np.eye(len(d)) for _ in theta.T])\n",
    "    #print(likelihood_cov)\n",
    "    likelihood_mean = theta.T @ d_offset.T\n",
    "    return [stable_multivariate_gaussian_logpdf(y, mean, cov) for mean, cov in zip(likelihood_mean, likelihood_cov)]\n",
    "def elbo_optimized(q_params, d, y_i, shape=3, batch_size = 500):\n",
    "    #print(\"Computing elbo for \", q_params, d, y_i)\n",
    "    mean, A = q_params\n",
    "    #values = np.random.multivariate_normal(np.zeros(shape), np.eye(shape), size=batch_size)\n",
    "    #theta = mean + A @ values.T\n",
    "    sample_values = np.random.multivariate_normal(np.zeros(3), np.eye(3), size=batch_size)\n",
    "    theta = np.expand_dims(mean, axis=1) + A @ sample_values.T\n",
    "    lik = log_likelihood_opt(y_i, theta, d)\n",
    "    mean_term = (1/batch_size) * np.sum(lik)\n",
    "    kld_term = KLD(mean, A, mean_prior, A_prior)\n",
    "    #print(lik[0])\n",
    "    \n",
    "    return mean_term - kld_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d7595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(theta_i, q_params):\n",
    "    mean, A = q_params\n",
    "    return stable_multivariate_gaussian_logpdf(theta_i, mean, A @ A.T)\n",
    "def log_posterior_grad(theta_i, y_i, d, q_params):\n",
    "    batch_size=500\n",
    "    \n",
    "    def training_hessian_inner(q_params):\n",
    "        return elbo_optimized(decode_q_params(q_params), d, y_i, batch_size=batch_size)\n",
    "    \n",
    "    def training_mixed_partials_inner(encoded_q):\n",
    "        def deep_inner(d):\n",
    "            return elbo_optimized(\n",
    "                    decode_q_params(encoded_q), \n",
    "                    decode_d(d), \n",
    "                    y_i, \n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "        #print(\"Encoded d %s\" % str(encode_d(d)))\n",
    "        #print(\"inner result: %s\" % str(encode_d(grad(lambda d: elbo(decode_q_params(encoded_q), d, y_i))(d))))\n",
    "        def decode_d_y(d_y):\n",
    "            size = d.shape[0] * d.shape[1]\n",
    "            return decode_d(d_y[0:size]), d_y[size:]\n",
    "        def encode_d_y(d, y):\n",
    "            return np.concatenate((encode_d(d), y_i))\n",
    "        return encode_d(\n",
    "            grad(\n",
    "                deep_inner\n",
    "            )(encode_d(d))\n",
    "        )\n",
    "    def training_mixed_partials_inner_2(encoded_d):\n",
    "        d = decode_d(encoded_d)\n",
    "        def deep_inner(encoded_q):\n",
    "            mean, A = decode_q_params(encoded_q)\n",
    "            return elbo_optimized((mean, A), d, y_i, batch_size=batch_size)\n",
    "        return grad(\n",
    "                deep_inner\n",
    "            )\n",
    "    \n",
    "    def inner_grad_q(encoded_q_params):\n",
    "        mean, A = decode_q_params(encoded_q_params)\n",
    "        return stable_multivariate_gaussian_logpdf(theta_i, mean, A @ A.T)\n",
    "    \n",
    "    \n",
    "    training_hessian = hessian(training_hessian_inner)\n",
    "    training_mixed_partials = jacobian(training_mixed_partials_inner)\n",
    "    encoded_q = encode_q_params(q_params)\n",
    "    latest_hessian = training_hessian(encoded_q) # 12 x 12\n",
    "    grad_log_q = grad(inner_grad_q)(encoded_q).reshape(1, len(encoded_q)) # 1 x 12\n",
    "    #alt_grad = -jacobian(lambda d: encode_q_params(get_posterior(decode_d(d), y_i)))(encode_d(d))\n",
    "    #alt_grad = -jacobian(lambda d: encode_q_params(get_posterior(decode_d(d), y_i)))(encode_d(d))\n",
    "    #return (grad_log_q @ alt_grad)[0]\n",
    "    #print(\"Alt grad: \", alt_grad)\n",
    "    for i in range(20):\n",
    "        if np.linalg.det(latest_hessian) != 0:\n",
    "            evaluated_partials = training_mixed_partials(encoded_q)\n",
    "            inv = np.linalg.inv(latest_hessian)\n",
    "            #print(\"New grad: \", (- inv @ evaluated_partials.T))\n",
    "            #return (grad_log_q @ alt_grad)[0]\n",
    "            return (grad_log_q @ (- inv @ evaluated_partials.T))[0]\n",
    "        latest_hessian = latest_hessian + 1e-8 * np.eye(latest_hessian.shape[0])\n",
    "    raise ValueError(\"Was not able to invert hessian\")\n",
    "def optimal_q(d, y_i, rate=0.1, decay=0.001, iters=100):\n",
    "    thetas = []\n",
    "    def objective_f(encoded_q):\n",
    "        q_params = decode_q_params(encoded_q)\n",
    "        return - elbo_optimized(q_params, d, y_i)\n",
    "    def callback(qi, g):\n",
    "        #print(\"Elbo: %f.05,\\t  grad: %f.05\" % (timer(lambda: -objective_f(qi), label=\"\\t ELBO\"), np.linalg.norm(g)))\n",
    "        return True\n",
    "        #print(\"\\t New ELBO iteration. mean=%s, A=%s\" % (str(mean), str(A)))\n",
    "    results = optimizer(\n",
    "        encode_q_params(\n",
    "            (mean_prior, A_prior)\n",
    "        ), \n",
    "        grad(objective_f), \n",
    "        callback=callback, \n",
    "        label=\"Optimizing for q\", \n",
    "        rate=rate,\n",
    "        iters=iters, \n",
    "        leave=False,\n",
    "        decay=decay\n",
    "    )\n",
    "    return decode_q_params(results)\n",
    "def MI_grad(d, N=10):\n",
    "    d_offset = augment_d(d)\n",
    "    size = d.shape[0]\n",
    "    theta_samples = np.random.multivariate_normal(mean_prior, A_prior @ A_prior, size=N)\n",
    "    sample_results = []\n",
    "    #elbos = []\n",
    "    optimal_qs = []\n",
    "    for i in tqdm(range(N), desc=\"Sampling over theta\", leave=False):\n",
    "        theta_i = theta_samples[i]\n",
    "        z_i = np.random.multivariate_normal(np.zeros(size), noise * np.eye(size))\n",
    "        def get_y_from_d(d):\n",
    "            d_offset = augment_d(d)\n",
    "            return theta_i @ d_offset.T + z_i\n",
    "        y_i = get_y_from_d(d)\n",
    "        q_params = get_posterior(d, y_i)\n",
    "        analytical_q = None\n",
    "        #data_sets.append((d, y_i, q_params, analytical_q))\n",
    "        #optimal_qs.append(q_params)\n",
    "        def log_posterior_grad_alt(theta_i, y_i, d, q_params):\n",
    "            return grad(\n",
    "              lambda xs_: -log_posterior(\n",
    "                  theta_i, \n",
    "                  get_posterior(decode_d(xs_), y_i)\n",
    "                  )\n",
    "              )(encode_d(d))\n",
    "        \n",
    "        result = np.exp(log_posterior_grad(q_params[0], y_i, d, q_params) - log_posterior(theta_i, q_params) * np.ones(20))\n",
    "        #print(result, log_posterior_grad_alt(theta_i, y_i, d, q_params) * (log_posterior(theta_i, q_params) + 1))\n",
    "        #result = log_posterior_grad_alt(theta_i, y_i, d, q_params) * (log_posterior(theta_i, q_params) + 1)\n",
    "        #print(\"Got result!\")\n",
    "        sample_results.append(result)\n",
    "        #elbos.append(elbo(q_params, d, y_i))\n",
    "    #optimal_params.append(optimal_qs)\n",
    "    #print(\"ELBO: \", str(np.mean(elbos)))\n",
    "    #print(\"MI_grad: \", str(np.mean(sample_results)))\n",
    "    #print(sample_results)\n",
    "    #print(\"sample results, mean 0\", str(np.mean(np.array(sample_results), axis=0).shape))\n",
    "    #print(encode_d(decode_d(np.mean(np.array(sample_results), axis=0), dim=3)[:, 0:2]))\n",
    "    return decode_d(np.mean(np.array(sample_results), axis=0), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04678b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48ee4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(x0, gradient, callback=lambda x, g: True, rate=1, decay=0.5, iters=1e1, label=None, leave=True, thres=0.1):\n",
    "    x = x0\n",
    "    for i in tqdm(range(int(iters)), desc=label, leave=leave):\n",
    "        r = rate * 1/(10+i*decay)\n",
    "        g = gradient(x)\n",
    "        x = -g * r + x\n",
    "        #if i > 0 and np.linalg.norm(g) < thres or np.linalg.norm(g) > 10:\n",
    "        #    break\n",
    "        if i % 1 == 0:\n",
    "            if not callback(x, g):\n",
    "                break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a61969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591f68154415472e930f920b90c69bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling over theta:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling over theta:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmuslovstad/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/numpy/core/_methods.py:236: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/tmp/ipykernel_16959/3907716183.py:5: RuntimeWarning: invalid value encountered in matmul\n",
      "  mean_theta = covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ y_i\n",
      "/tmp/ipykernel_16959/3907716183.py:6: RuntimeWarning: invalid value encountered in matmul\n",
      "  cov_theta = covariance_prior - covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ d @ covariance_prior\n",
      "/home/rasmuslovstad/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/numpy/linalg/linalg.py:2079: RuntimeWarning: invalid value encountered in slogdet\n",
      "  sign, logdet = _umath_linalg.slogdet(a, signature=signature)\n",
      "/home/rasmuslovstad/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/tracer.py:48: RuntimeWarning: invalid value encountered in matmul\n",
      "  return f_raw(*args, **kwargs)\n",
      "/home/rasmuslovstad/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/numpy/linalg/linalg.py:2139: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling over theta:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling over theta:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3765185666a445bbae333fcef9a7d6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling over theta:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m dis\u001b[38;5;241m.\u001b[39mappend(d0)\n\u001b[1;32m     14\u001b[0m mis\u001b[38;5;241m.\u001b[39mappend(MI(d0, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode_d\u001b[49m\u001b[43m(\u001b[49m\u001b[43md0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m dis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(dis)\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36moptimizer\u001b[0;34m(x0, gradient, callback, rate, decay, iters, label, leave, thres)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(iters)), desc\u001b[38;5;241m=\u001b[39mlabel, leave\u001b[38;5;241m=\u001b[39mleave):\n\u001b[1;32m      4\u001b[0m     r \u001b[38;5;241m=\u001b[39m rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m+\u001b[39mi\u001b[38;5;241m*\u001b[39mdecay)\n\u001b[0;32m----> 5\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mg \u001b[38;5;241m*\u001b[39m r \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#if i > 0 and np.linalg.norm(g) < thres or np.linalg.norm(g) > 10:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#    break\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mg\u001b[0;34m(encoded_d)\u001b[0m\n\u001b[1;32m      7\u001b[0m d \u001b[38;5;241m=\u001b[39m decode_d(encoded_d)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#g_ = grad(lambda d: MI(d, N=50))(d)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#g_ = MI(d, N=100)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m g_ \u001b[38;5;241m=\u001b[39m \u001b[43mMI_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m mis\u001b[38;5;241m.\u001b[39mappend(g_)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mencode_d(g_)\n",
      "Cell \u001b[0;32mIn[13], line 110\u001b[0m, in \u001b[0;36mMI_grad\u001b[0;34m(d, N)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_posterior_grad_alt\u001b[39m(theta_i, y_i, d, q_params):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad(\n\u001b[1;32m    104\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m xs_: \u001b[38;5;241m-\u001b[39mlog_posterior(\n\u001b[1;32m    105\u001b[0m           theta_i, \n\u001b[1;32m    106\u001b[0m           get_posterior(decode_d(xs_), y_i)\n\u001b[1;32m    107\u001b[0m           )\n\u001b[1;32m    108\u001b[0m       )(encode_d(d))\n\u001b[0;32m--> 110\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[43mlog_posterior_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m log_posterior(theta_i, q_params) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#print(result, log_posterior_grad_alt(theta_i, y_i, d, q_params) * (log_posterior(theta_i, q_params) + 1))\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#result = log_posterior_grad_alt(theta_i, y_i, d, q_params) * (log_posterior(theta_i, q_params) + 1)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m#print(\"Got result!\")\u001b[39;00m\n\u001b[1;32m    114\u001b[0m sample_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[13], line 55\u001b[0m, in \u001b[0;36mlog_posterior_grad\u001b[0;34m(theta_i, y_i, d, q_params)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mdet(latest_hessian) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m         evaluated_partials \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_mixed_partials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(latest_hessian)\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m#print(\"New grad: \", (- inv @ evaluated_partials.T))\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m#return (grad_log_q @ alt_grad)[0]\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/wrap_util.py:20\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munary_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43munary_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/differential_operators.py:64\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m jacobian_shape \u001b[38;5;241m=\u001b[39m ans_vspace\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m+\u001b[39m vspace(x)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     63\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(vjp, ans_vspace\u001b[38;5;241m.\u001b[39mstandard_basis())\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m, jacobian_shape)\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/numpy/numpy_wrapper.py:88\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# we need it here because we want to re-implement stack in terms of the\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# primitives defined in this file\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [array(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/numpy/numpy_wrapper.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# we need it here because we want to re-implement stack in terms of the\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# primitives defined in this file\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [array(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/core.py:14\u001b[0m, in \u001b[0;36mmake_vjp.<locals>.vjp\u001b[0;34m(g)\u001b[0m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvjp\u001b[39m(g): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_node\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/core.py:21\u001b[0m, in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m toposort(end_node):\n\u001b[1;32m     20\u001b[0m     outgrad \u001b[38;5;241m=\u001b[39m outgrads\u001b[38;5;241m.\u001b[39mpop(node)\n\u001b[0;32m---> 21\u001b[0m     ingrads \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m parent, ingrad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mparents, ingrads):\n\u001b[1;32m     23\u001b[0m         outgrads[parent] \u001b[38;5;241m=\u001b[39m add_outgrads(outgrads\u001b[38;5;241m.\u001b[39mget(parent), ingrad)\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/core.py:78\u001b[0m, in \u001b[0;36mdefvjp.<locals>.vjp_argnums.<locals>.<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     76\u001b[0m     vjp_0 \u001b[38;5;241m=\u001b[39m vjp_0_fun(ans, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m     vjp_1 \u001b[38;5;241m=\u001b[39m vjp_1_fun(ans, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m g: (\u001b[43mvjp_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m, vjp_1(g))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     vjps \u001b[38;5;241m=\u001b[39m [vjps_dict[argnum](ans, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m argnum \u001b[38;5;129;01min\u001b[39;00m argnums]\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/numpy/numpy_vjps.py:418\u001b[0m, in \u001b[0;36mmatmul_vjp_0.<locals>.<lambda>\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    416\u001b[0m A_meta \u001b[38;5;241m=\u001b[39m anp\u001b[38;5;241m.\u001b[39mmetadata(A)\n\u001b[1;32m    417\u001b[0m B_ndim \u001b[38;5;241m=\u001b[39m anp\u001b[38;5;241m.\u001b[39mndim(B)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m g: \u001b[43mmatmul_adjoint_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_ndim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/numpy/numpy_vjps.py:385\u001b[0m, in \u001b[0;36mmatmul_adjoint_0\u001b[0;34m(B, G, A_meta, B_ndim)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul_adjoint_0\u001b[39m(B, G, A_meta, B_ndim):\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43manp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# A_ndim == B_ndim == 1\u001b[39;00m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m unbroadcast(G \u001b[38;5;241m*\u001b[39m B, A_meta)\n\u001b[1;32m    387\u001b[0m     _, A_ndim, _, _ \u001b[38;5;241m=\u001b[39m A_meta\n",
      "File \u001b[0;32m~/Programs/assignments/pocs/python/venv/lib/python3.10/site-packages/autograd/tracer.py:61\u001b[0m, in \u001b[0;36mnotrace_primitive.<locals>.f_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f_raw)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     60\u001b[0m     argvals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(getval, args)\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dis = []\n",
    "mis = []\n",
    "def callback(di, g):\n",
    "    dis.append(decode_d(di))\n",
    "    return True\n",
    "def g(encoded_d):\n",
    "    d = decode_d(encoded_d)\n",
    "    #g_ = grad(lambda d: MI(d, N=50))(d)\n",
    "    #g_ = MI(d, N=100)\n",
    "    g_ = MI_grad(d)\n",
    "    mis.append(g_)\n",
    "    return -encode_d(g_)\n",
    "dis.append(d0)\n",
    "mis.append(MI(d0, N=30))\n",
    "optimizer(encode_d(d0), g, rate=1, decay=0.0001, iters=100000, callback=callback)\n",
    "dis = np.array(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94857401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]),\n",
       " array([[inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf],\n",
       "        [inf, inf]]),\n",
       " array([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]]),\n",
       " array([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]]),\n",
       " array([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd089a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(mis)), mis)\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Mutual information\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_mutual_information(d, dim=3): # From week 2\n",
    "    covariance_prior = A_prior @ A_prior.T\n",
    "    var_y = noise * np.eye(d.shape[0])\n",
    "    cov_posterior = covariance_prior - covariance_prior @ d.T @ np.linalg.inv(var_y + d @ covariance_prior @ d.T) @ d @ covariance_prior\n",
    "    #val = 0.5 * np.log(np.linalg.det(2*np.pi*np.e*cov_posterior)) - 0.5 * np.log(np.linalg.det(2*np.pi*np.e*covariance_prior))\n",
    "    return 0.5 * np.log(np.linalg.det(2*np.pi*np.e*cov_posterior))\n",
    "plt.plot(np.arange(0, len(dis)), np.array([MI(d, N=10) for d in dis]))\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Mutual information\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, len(dis)), np.array([np.linalg.norm(d) for d in dis]))\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Norm of d\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Norm of d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_grad(dis[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "di = dis[i]\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "plt.scatter(di[:, 0], di[:, 1])\n",
    "plt.title(\"$d$ at iteration %d\" % (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = np.array(dis)\n",
    "for p in range(d0.shape[0]):\n",
    "    plt.scatter(dis[0, p, 0], dis[0, p, 1], c=\"blue\") # start\n",
    "    plt.scatter(dis[-1, p, 0], dis[-1, p, 1], c=\"orange\") # end\n",
    "    plt.plot(dis[:, p, 0], dis[:, p, 1], alpha=0.5)\n",
    "plt.title(\"Points move per iteration (blue is start, orange is end)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e377e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection.mutual_info_regression(d0, augment_d(d0) @ mean_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e093b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb158d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
