\section{Week 1}
\subsection{Objective}
For week 1, we're going to explore the linear regression problem from a Bayesian perspective. 
First we are going to state essential model assumptions, from which we can examine the consequences of these using Bayes' rule.
This will be used to see how we can incorporate previous knowledge into the model and see how observing new data will change our model.
While this is apparent for any regression model, we will also demonstrate how we can actually derive an exact posterior parameter distribution in the case of linear regression.\\
In the end, we are going to implement a simple linear regression model in Python, and explore how the prior model assumptions affect our resulting model.
% The objective of this week is to explore the basics of probabilistic machine learning by implementing a Bayesian linear regression model 
% and explore how it is affected by its different hyperparameters like choice of prior as well as the variance and quantity of data points.
% \todo{Notes from meeting: Make 2D plot over parameter space that shows how the posterior moves from the prior. Update from Oswin's notes}
\subsection{Theory}
\subsubsection{The regression problem and Bayes' rule}
First, we are going to state the regression problem in general: 
Given a dataset $\mathcal{D} = (\B{d}, \B{y})$, where $\B{d}\in \mathbb{R}^{\ell \times d}$ and $\B{y} \in \mathbb{R}^{\ell}$, 
we wish to find some function $\phi_\theta \in \mathbb{R}^{\ell \times d} \rightarrow \mathbb{R}^{\ell}$ such that
\begin{equation}
  \label{eq:general-regression}
\phi(\B{d}) \approx \B{y}
\end{equation}
where $\phi$ takes some vector of parameters $\theta$.\\
In the Bayesian approach, we are going to describe the probability distribution of $\B{y}$ with no additional information as $p(\B{y})$, called the \textit{marginal}.
If we have observed any data $\B{d}$ and have some set of parameters $\theta$ such that equation \ref{eq:general-regression} is upheld, then we have additional information about the distribution of $\B{y}$.
This will be described as the \textit{likelihood} $p(\B{y} | \B{d}, \B{\theta})$.\\
We might have some prior information on how the parameters $\B{\theta}$ are distributed - perhaps from prior experiments, domain knowledge or qualified guessing. This can be encoded into the \textit{prior} distribution $p(\B{\theta})$.
Given a likelihood, a marginal, and a prior, we can use Bayes' rule to find the \textit{posterior} distribution of $\B{\theta}$ given the data $\B{d}$ and target values $\B{y}$:
\begin{equation}
  \label{eq:bayes-rule}
  p(\B{\theta} | \B{d}, \B{y}) = \frac{p(\B{y} | \B{d}, \B{\theta})p(\B{\theta})}{p(\B{y})}
\end{equation}
What the posterior distribution $p(\B{\theta} | \B{d}, \B{y})$ models is the \textit{change of belief} from the prior model $p(\theta)$ given the \textit{evidence} $\B{y}$, $\B{d}$\cite{krause22}.
It gives us a new best bet for the parameters $\B{\theta}$, which we can use to produce a better $\phi$ for equation $\ref{eq:general-regression}$.
%we wish to find a set of model parameters $\theta \in \mathbb{R}^{\ell \times d}$ such that 
\subsubsection{The linear regression problem}
In the case of linear regression, we assume that $\phi$ is a linear function, i.e. $\phi(\B{d}) = \B{d}\B{\theta}$ where $\B{\theta}\in \mathbb{R}^{d}$. 
It is also common to augment the dataset with an additional dimension set to 1, such that $\phi$ also models the intercept such that $\B{d} \in \mathbb{R}^{\ell \times d + 1}$ and $\theta \in \mathbb{R}^{d + 1}$. In this case $\theta_{d+1}$ will then be the intercept.\\

We are going to assume that the each target value $\B{y}$ can accurately be described as
\begin{equation}
  \label{eq:likelihood-generating}
  \B{y}^{(i)} = \B{d}^{(i)}\theta + \epsilon
\end{equation}
for some $\B{d}$, $\theta$, $\epsilon$, where $\epsilon \sim \mathcal{N}(0, \sigma^2_{\B{y}})$ is some normally distributed noise term.
Thus, the likelihood distribution can be described like a normal distribution.
\begin{equation}
  \label{eq:likelihood-single}
  p(\textbf{y}^{(i)} | \B{d}^{(i)}, \B{\theta}) = \mathcal{N}(\B{y}^{(i)}; \B{d}^{(i)}\B{\theta}, \sigma^2_{\B{y}})
\end{equation}
Assuming that the target values are independent, it must follow that
\begin{equation}
  \label{eq:likelihood}
  p(\textbf{y} | \B{d}, \B{\theta}) = \prod_{i=0}^\ell p(\textbf{y}^{(i)} | \B{d}^{(i)}, \B{\theta}) = \mathcal{N}(\B{y}; \B{d}\B{\theta}, \sigma^2_{\B{y}}I_\ell)
\end{equation}
When modelling the prior information, one could choose to model the parameter distribution as a multivariate gaussian distribution as well
\begin{equation}
  \label{eq:prior}
  p(\B{\theta}) = \mathcal{N}(\theta; \mu_{\theta}, \Sigma_\theta)
\end{equation}
where $\mu_\theta$, $\Sigma_\theta$ are chosen by the model designer.\\
The last term of equation \ref{eq:bayes-rule} is the marginal distribution $p(\B{y})$, which is the probability of observing the target values $\B{y}$ without any additional information. 
This term is difficult to make any reasonable assumptions about, and can thus hard to compute in practice.
Lukily, it mostly acts as a normalization term that makes sure that the posterior distribution integrates to 1, so it can for many use cases be safely ignored.
As we will see, for a linear regression problems with these assumptions, we will indeed not need it.
\subsubsection{An analytical solution for the posterior distribution}
Let us outline a possible derivation for the posterior distribution: Given the likelihood and prior, we can describe the joint distribution $p(\theta, \B{y} | \B{d})$ as
\begin{equation}
  \label{eq:joint}
  p(\theta, \B{y} | \B{d}) = p(\B{y} | \B{d}, \theta)p(\theta | \B{d}) = p(\B{y} | \B{d}, \theta)p(\theta)
\end{equation}
where the last equality is due to the model parameters being independent from the controlled data points. \\
From this point, we will need to condition the joint distribution on $\B{y}$:
\begin{equation}
  \label{eq:joint-conditioned}
  p(\theta | \B{y}, \B{d}) = \frac{p(\theta, \B{y} | \B{d})}{p(\B{y})}
\end{equation}
To perform these steps, we are going to use some useful lemmas about the multivariate normal distribution.
\begin{lemma}
  \label{lemma:block-conditional}
  Let $\B{X} \sim \mathcal{N}(\B{\mu}, \Sigma)$ be a multivariate normal random variable. We can regard these variables in block notation:
  \begin{equation}
  \B{X}=\begin{bmatrix}[c]
    \B{X}_1\\
    \hline
    \B{X}_2
  \end{bmatrix}, \quad \mu = \begin{bmatrix}[c]
    \mu_{1}\\
    \hline
    \mu_{2}
  \end{bmatrix},\quad \Sigma=\begin{bmatrix}[c|c]
  \Sigma_{11} & \Sigma_{12}\\
    \hline
    \Sigma_{21} & \Sigma_{22}
  \end{bmatrix}
\end{equation}
  then we must have
  \begin{equation}\mu_{2|1}=\mu_2 + \Sigma_{21}\Sigma_{11}^{-1}(X_1 - \mu_1),\quad \Sigma_{2|1} = \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{21}^T\end{equation}
\end{lemma}
\begin{proof}
  See \citet{krause22} \todo{implement proof}
\end{proof}
\begin{lemma}
  \label{lemma:block-joint}
  For random variables $\B{X} \sim \mathcal{N}(\B{\mu}_\B{X}, \Sigma_\B{X})$, $\B{Y}|\B{X} \sim \mathcal{N}(\B{\mu}_\B{Y} + A\B{X}, \Sigma_\B{Y})$,
  for some $A$, the joint distribution $p(\B{X}, \B{Y})$ is given by
  \begin{equation}
    \begin{bmatrix}[c]
      \B{X} \\
      \hline
      \B{Y}
    \end{bmatrix}
    =
    \mathcal{N}\left(
      \begin{bmatrix}[c]
        \B{\mu}_\B{X} \\
        \hline
        \B{\mu}_\B{Y} + A\B{\mu}_\B{X}
      \end{bmatrix},
      \begin{bmatrix}[c | c]
        \Sigma_\B{X} & \Sigma_\B{X}A^T \\
        \hline
        A\Sigma_\B{X} & A\Sigma_\B{X}A^T + \Sigma_\B{Y}
      \end{bmatrix}
    \right)
  \end{equation}
\end{lemma}
\begin{proof}
  Follows from Lemma \ref{lemma:block-conditional} and the definition of the multivariate normal distribution.
\end{proof}
If we regard the likelihood from equation \ref{eq:likelihood}, we can see that it is only dependent of $\theta$ in its mean-term.
Thus we can use Lemma \ref{lemma:block-joint} to get the joint distribution $p(\theta, \B{y} | \B{d})$:
\begin{equation}
  \begin{bmatrix}[c]
    \theta \\
    \hline
    \B{y}
  \end{bmatrix}
  =
  \mathcal{N}\left(
    \begin{bmatrix}[c]
      \mu_\theta \\
      \hline
      \B{d}\theta + \B{d}\mu_\theta
    \end{bmatrix},
    \begin{bmatrix}[c | c]
      \Sigma_\B{\theta} & \Sigma_\B{\theta}\B{d}^T \\
      \hline
      \B{d}\Sigma_\B{\theta} & \B{d}\Sigma_\B{\theta}\B{d}^T + \sigma^2_\B{y} I_\ell
    \end{bmatrix}
  \right)
\end{equation}
To get the posterior distribution, we will need to use the conditioning Lemma \ref{lemma:block-conditional} on a reordered term:
\begin{equation}
  \begin{bmatrix}[c]
    \B{y}\\
    \hline
    \theta 
  \end{bmatrix}
  =
  \mathcal{N}\left(
    \begin{bmatrix}[c]
      \B{d}\theta + \B{d}\mu_\theta\\
      \hline
      \mu_\theta
    \end{bmatrix},
    \begin{bmatrix}[c | c]
      \B{d}\Sigma_\B{\theta}\B{d}^T + \sigma^2_\B{y} I_\ell &     \B{d}\Sigma_\B{\theta} \\
      \hline
      \Sigma_\B{\theta}\B{d}^T & \Sigma_\B{\theta}
    \end{bmatrix}
  \right)
\end{equation}
With the conditioning lemma, we get the posterior distribution $p(\theta | \B{y}, \B{d}) = \mathcal{N}(\theta; \mu_{\theta| \B{y}, \B{d}}, \Sigma_{\theta | \B{y}, \B{d}})$:
\begin{equation}
  \label{eq:posterior-mean}
  \mu_{\theta | \B{y}, \B{d}} = \mu_\theta + \Sigma_\theta \B{d}^T (\B{d} \Sigma_\theta \B{d}^T + \sigma^2_\B{y} I_\ell)^{-1}(\B{y} - \B{d}\theta + \B{d}\mu_\theta)
\end{equation}
\begin{equation}
  \label{eq:posterior-cov}
  \Sigma_{\theta | \B{y}, \B{d}} = \Sigma_\theta - \Sigma_\theta \B{d}^T (\B{d} \Sigma_\theta \B{d}^T + \sigma^2_\B{y} I_\ell)^{-1}\B{d}\Sigma_\theta
\end{equation}
Thus we have an analytical expression for the posterior distribution.
\subsubsection{Using the posterior to optain model parameters}
From this distribution, we can optain model parameters in several ways. 
A common choice is to pick the $\theta$ that maximizes the posterior. This is called the \textit{Maximum-a-posteriori} (MAP) estimate, and will for a normal distribution just be the posterior mean $\mu_{\theta | \B{y}, \B{d}}$.
For other cases than linear regression, one can also use a mean over samples of $\theta$ from the posterior distribution, but for our case, this will converge against the posterior mean $\mu_{\theta | \B{y}, \B{d}}$.
Another solution is to define a distribution called the \textit{posterior predictive} distribution, that for some new data points $\B{d}_{\textrm{new}}$ computes the corresponding $\B{y}_{\textrm{new}}$:
\begin{equation}p(\B{y}_{\textrm{new}} | \B{d}_{\textrm{new}}, \B{d}, \B{y}) = \int p(\theta | \B{y}, \B{d}) p(\B{y}_{\textrm{new}} | \theta, \B{d}_\textrm{new})d\theta\end{equation}
Integrals like these are usually solved by sampling - examples of this will be seen later. For now, we can use Lemma \ref{lemma:block-joint} to get
\begin{equation}p(\B{y}_{\textrm{new}} | \B{d}_{\textrm{new}}, \B{d}, \B{y}) = \mathcal{N}(\B{y}_\textrm{new}; \B{d}_\textrm{new}^T\mu_{\theta|\B{y}, \B{d}}, \sigma^2_\B{y} + \B{d}_\textrm{new}^T\Sigma_{\theta | \B{y}, \B{d}}\B{d}_\textrm{new})\end{equation}
Having the posterior predictive allows us to both sample a $\B{y}$ for a given $\B{d}$, choose the $\B{y}$ that maximises the distribution, and to give us some measure of the inherent uncertancy in the model \cite{krause22}.
\subsubsection{Linear regression being invariant to translation}
A relevant side-note worth exploring is the effect of the choice of prior mean $\mu_\theta$ on the posterior mean $\mu_{\theta | \B{y}, \B{d}}$.

\todo{Finish this section with information from Oswin - and edit posterior predictive if needed}

\subsection{Implementation}
Implementing a Bayesian linear regression model is fairly simple.
To begin with, one needs to decide on a prior distribution for $\theta$, as well as what the variance of the noise $\sigma^2_\B{y}$ should be used to generate the example data.
In the real world, $\sigma^2_\B{y}$ would be measured from observed data, and the prior would be chosen based on the expected distribution of $\theta$ based on as much prior knowledge as possible
, but for our toy representation, we are going to play around with many different possible priors and noise parameters.
For a toy implementation, one can start by generating some data points $\B{d}$ and noise samples $\epsilon$ as well as deciding on some true, underlying weight parameters $\theta_{\textrm{true}}$.
Then we can compute $\B{y}$ as in equation \ref{eq:likelihood-generating}. 
From this, we can simply implement the posterior distribution as in equation \ref{eq:posterior-mean} and \ref{eq:posterior-cov}.
In Python, this can be done as follows:
\begin{minted}{python}
def posterior_distribution(theta, d, y):
    mu = cov_prior @ d.T @ np.linalg.inv(d @ cov_prior @ d.T + noise * np.eye(l)) @ y
    cov = cov_prior - cov_prior @ d.T @ np.linalg.inv(d @ cov_prior @ d.T + noise * np.eye(l)) @ d @ cov_prior
    return stats.multivariate_normal.pdf(theta, mean=mu, cov=cov)
\end{minted}

\subsection{Results}
An example of how 20 samples of $\theta$ from the posterior changes with the size of the dataset can be seen in Figure \ref{fig:fixed-prior}.
As one can see, the posterior becomes better at estimating the true $\theta$ as the number of data points increases,
since the lines both become more similar and closer to the mean, as well as how they describe the data better. \\
An example of how the posterior changes from the prior can be seen in figure \ref{fig:random-prior}, where 40 random prior means and covariances are plotted alongside their respective posterior.
It can be seen that even when the priors are very different, the posterior usually ends up quite close to the true weight.
An example of how the posterior predictive distribution changes is seen in figure \ref{fig:posterior-predictive}.
As one can see, the distribution becomes less confident as the noise increases. \\
Thus we've seen that the Bayesian linear regression model is able to learn the true weight parameters from data, and that it is able to do so even when the prior is very different from the true weight parameters. Now, we will move on to regarding the Bayesian Optimal Design problem.
\todo{Make plots nicer, add "true" line for reference, add legend}
\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{assets/week1/fixed-prior-20-samples.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{assets/week1/fixed-prior-40-samples.png}
\end{subfigure}\\
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{assets/week1/fixed-prior-200-samples.png}
\end{subfigure}%
\caption{The model trained on data sets with 20, 40 and 200 data points respectively with a fixed prior. The blue line is $\mu_{\theta|\mathcal{D}}$.}
\label{fig:fixed-prior}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=0.7\textwidth]{assets/week1/random-priors.png}
  \caption{Plot showing how the posterior mean changes compared to the prior mean.}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{assets/week1/random-priors-plot.png}
  \caption{Prior mean samples}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{assets/week1/random-priors-posteriors.png}
  \caption{Posterior mean samples}
\end{subfigure}
\caption{Priors vs Posteriors for 40 randomly sampled priors on the same data set. Note that the variance is due to random prior covariances.}
\label{fig:random-prior}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=0.5\textwidth]{assets/week1/noise-1.png}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{assets/week1/noise-10.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{assets/week1/noise-100.png}
\end{subfigure}
\caption{Posterior predictive for different noise levels}
\label{fig:posterior-predictive}
\end{figure}
